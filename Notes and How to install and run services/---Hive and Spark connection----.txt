---Hive and Spark connection----

1. Set up Spark_home in bashrc

hadoop@AcerPredator:~$ sudo nano ~/.bashrc

export SPARK_HOME=/usr/local/bin/spark
export PATH=$PATH:$SPARK_HOME/bin
export SPARK_LOCAL_HOSTNAME=localhost

2. just change this property in hive-site.xml

<property>
    <name>hive.metastore.uris</name>
    <value>thrift://localhost:9083</value>
    <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
</property>


3. copy hive-site.xml , hdfs-site.xml and core-site.xml, to the location where spark/conf directory 

spark location: /usr/local/spark/conf

4. if you are getting so much warning in pyspark session:

example:

23/09/08 11:11:11 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.am.registry does not exist
23/09/08 11:11:11 WARN HiveConf: HiveConf of name hive.druid.overlord.address.default does not exist
23/09/08 11:11:11 WARN HiveConf: HiveConf of name hive.optimize.remove.sq_count_check does not exist
23/09/08 11:11:11 WARN HiveConf: HiveConf of name hive.server2.webui.enable.cors does not exist
23/09/08 11:11:11 WARN HiveConf: HiveConf of name hive.vectorized.row.serde.inputformat.excludes does not exist
23/09/08 11:11:11 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.size does not exist

solve:
>>> # To change the logging level to ERROR (suppressing WARN messages)
>>> spark.sparkContext.setLogLevel("ERROR")
>>> # To turn off logging
>>> spark.sparkContext.setLogLevel("OFF")

Result: >>> a = spark.sql("show databases").show()
+---------+
|namespace|
+---------+
| commerce|
|    covid|
|  default|
+---------+

** to drop database having object 

Use DROP SCHEMA ... CASCADE to drop the schema and all its objects.
>>> spark.sql("DROP SCHEMA ridesharing CASCADE")

dags_folder = /home/saksham/airflow/dags
spark_home = /usr/local/spark

load_examples = False
plugins_folder = /home/saksham/airflow/plugins

sql_alchemy_conn = mysql://saksham:password@localhost:3306/airflow



 
